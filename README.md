*****************************
***Summary of "Virtual Baby"***
*****************************
Virtual baby was designed as a way to better understand how parents perceive threats in the environment.
This task was designed in such a way as to threaten a virtual infant and then measure how parents detected and perceived those threats. 
In the game, subjects will see a crawling infant near an active road with cars. 
The goal of participants is twofold: 

1. Make sure their virtual infant doesn't get hit by oncoming traffic
2. Wave down cars so that someone may stop and help them

These goals are intentionally at odds with one another to also see how parents are dividing their attention. 
The main *dependent variables* in this experiment are reaction time to detect oncoming traffic and estimated speed judgements of passing cars.
*Independent variables* and additional dependent variables will vary by project version.

The P.I. on this project is Emma Murrugarra (eam422@cornell.edu) working with Michael Goldstein (mhg26@cornell.edu) in the B.A.B.Y. Lab at Cornell University. 
More detailed descriptions and motivations for this project are written about in Murrugarra & Goldstein (2024, submitted) and Murrugarra & Goldstein (2023, submitted).


****************
***File Navigation***
****************

#### **Project Versions**
Over time, the goals of Virtual Baby have changed. As such, there are multiple versions of this game publically published. 

* **V1.0** - This is the original version of Virtual Baby. This project features only a crawling infant condition before and after a no-infant control condition. This project was build as a 3D-UI and was administered remotely online. 

* **V2.0** - The version of Virtual Baby accomodates 2 additional infant locomotion types. The infant will randomly present as a walker, a crawler, or a pre-crawler. Participants will see all 3 infant types in addition to the no-infant control condition.

A Video Demonstration of V2.0 can be found here: https://youtu.be/zQ6RGW2WfkU

* **V2.1** - This is the Arduino-based modification to be used in conjunction with V2.0. This project contains programs for collecting Galvanic Skin Resistance (GSR) from an Arduino microcontroller with a Seeed-Grove GSR sensor. This project is separate from V2.0 and does not contain Unity integration.

* **V2.2** - This version of Virtual Baby uses similar scene structure as V2.0, but presents a Robot and Dog model rather than different locomoting infants. The model will randomly present as a crawling infant, a walking adult dog, or a mobile toy robot. Participants will see all 3 model types in addition to the no-model control condition.

* **V3.0** - This is a modified version of Virtual Baby suited for Virtual Reality. The project was designed for a HTC Vive Pro Eye headset and contains eye- and periphery-tracking output. Participants will see both a VR tutorial that orients them to game mechanics, as well as a crawing infant, no-infant, and robot condition. 

* **Data Analysis** - Raw  data output for all study versions as well as R data analysis scripts are included. Data includes output from Unity, GSR-sensors, HTC Vive Pro Eye headsets, and anonymized survey results. Data does not include output such as screen-recordings or participant video recordings.



#### **Publicly Available Builds**
As of 01.01.2023, the game is no longer being hosted on itch.io. Please email the authors for a publicly available
and completed builds for MAC and Windows install. 

*****************************************************************************************************
*****If you use or reference any of the materials in this repository, you can cite this work at:*****
*****************************************************************************************************

_**For materials**_

Murrugarra, E. (2024). Virtual Baby â€“ Parental Perception [Software]. GitHub. https://github.coecis.cornell.edu/BabyLab/VirtualBaby-ParentPerception

_**For V2.0 or V2.2**_

Murrugarra, E. & Goldstein, M.H. (2024). *Perceptual Parasitism: How infants change the way we see the world.* [Unpublished manuscript]. Department of Psychology, Cornell University.

_**For V2.1**_

Murrugarra, E. & Goldstein, M. H. (2024). *How we perceive the world around babies: Arousal moderates information-processing of infantile cues.* [Paper presentation]. IEEE International Conference on Development and Learning(ICDL) Austin, Texas.

****************
***Devopment Notes***
****************

This game was designed in Unity Version 2018.4.23f1 (free). It is recommended that you open the project in Unity using this version. For a completed build that does not require opening within Unity (best if you do not intend to make changes or develop the task further), please email the authors.

#### **Game Objects**
* All assets are free and publicly available, or were otherwise crafted by the authors. 
* All crafted models/animations were done in Blender (free; recommended). 

#### **Asset Organization** 
Many of the assets included in the projects are outdated or are not longer being used; often these are from previous drafts of the project and need to be carefully cleaned out. Optimization via asset organization has been de-prioritized for this repository. **Thus, it is recommended that novice users request a copy of the completed build from the authors.**

